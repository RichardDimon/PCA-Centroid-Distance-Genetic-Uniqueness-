{
  ### Input Parameters #####
setwd("C:/Users/dimon/rrspecies/Neolitsea_dealbata")

#"Diploglottis_australis"


library(patchwork)
library(gridExtra)
library(RColorBrewer)
library(circlize) 
library(scatterpie)
library(tanggle)
library(RSplitsTree)
library(qpdf)
# library(ggmap)
# library(lubridate)
# install.packages("ggplot2") #different version of ggplot affect the LEA plots
library(ggplot2)
library(dplyr)
library(data.table)
library(ggrepel)
library(tidyverse)
# library(ggsn)
library(ggspatial)
library(readxl)
library(heatmaply)
library(circlize)
library(ComplexHeatmap)
library(igraph)
library(SNPRelate)
library(geosphere)
library(reshape2)
library(vegan)
# library(ggforce)
library(openxlsx)
# library(stringr)
# library(pracma)
library(ggpubr)
library(RRtools)
library(ggthemes)
library(RColorBrewer)
library(ozmaps)
library(adegenet)
library(tidyr)
library(diveRsity)
library(RRtools)
library(LEA)


theme_set(theme_few())



setup_variables <- read.xlsx("0_setup_variables.xlsx", colNames = TRUE)
maindir <- setup_variables[1, 2]
species <- setup_variables[2, 2]
dataset <- setup_variables[3, 2]
RandRbase <- ""
raw_meta_path <- setup_variables[4, 2]
species_col_name <- setup_variables[5, 2]
site_col_name <- setup_variables[6, 2] # this is the equivalent of analysis
site_distances <- setup_variables[7, 2] %>% as.numeric()
remove_pops_less_than_n5 <- setup_variables[8, 2]
downsample <- setup_variables[9, 2]
samples_per_pop <- setup_variables[10, 2] %>% as.numeric()
locus_miss <- setup_variables[11, 2] %>% as.numeric()
sample_miss <- setup_variables[12, 2] %>% as.numeric()
maf_val <- setup_variables[13, 2] %>% as.numeric()
clonal_threshold <- setup_variables[14, 2] %>% as.numeric()
custom_meta <- setup_variables[15, 2] 

setwd(maindir)

topskip   <- 6
nmetavar  <- 20

subsubdirectories <- c(paste0("outputs_",site_col_name,"_",species_col_name,"/plots"),
                       paste0("outputs_",site_col_name,"_",species_col_name,"/tables"),
                       paste0("outputs_",site_col_name,"_",species_col_name,"/r_files"))

# Check and create subdirectories if they don't exist
for (subdir in subsubdirectories) {
  
  subdirectory_path <- file.path(paste0(species,'/',subdir))
  
  if (!dir.exists(subdirectory_path)) {
    dir.create(subdirectory_path, recursive = TRUE)
    cat(paste("Created directory:", subdirectory_path, "\n"))
  } else {
    cat(paste("Directory already exists:", subdirectory_path, "\n"))
  }
}
devtools::source_url("https://github.com/eilishmcmaster/SoS_functions/blob/main/sos_functions.R?raw=TRUE")

# note that this loads scripts generated by SY to automate / make RnR summary reports from DArTseq data. Should probably be put onto Git
# load("PSFsaved_scripts4autom2.RData")
### Import and QC ####

#Load data and quality filter loci and samples
d1        <- new.read.dart.xls.onerow(RandRbase,species,dataset,topskip, euchits=FALSE, altcount=TRUE)
qc1       <- report.dart.qc.stats(d1, RandRbase, species, dataset, threshold_missing_loci = 0.8)
#fix(report.dart.qc.stats) # need to add species variable to the qc_dir and anything else after the paste(basedir, function)

d2        <- remove.poor.quality.snps(d1, min_repro=0.96, max_missing=locus_miss)
qc2       <- report.dart.qc.stats(d2, RandRbase, species, dataset)

d3        <- sample.one.snp.per.locus.random(d2, seed=214241)
qc3       <- report.dart.qc.stats(d3, RandRbase, species, dataset)

#Load meta data and attach to dart data
m1        <- read.meta.data.full.analyses.df(d3, RandRbase, species, dataset)

m1$analyses[,site_col_name] <- gsub(" ", "_", m1$analyses[,site_col_name]) #this makes sure any spaces are replace by underscore

write.table(data.frame(sample=d3$sample_names[!(d3$sample_names %in% m1$sample_names)]), 
            paste0(species,"/outputs_",site_col_name,"_",species_col_name,"/tables/samples_in_dart_not_in_meta.tsv"), sep="\t", row.names = FALSE)

dm        <- dart.meta.data.merge(d3, m1)

# remove samples that are NA for site variable 
samples_with_site_variable <- dm$sample_names[!is.na(dm$meta$analyses[,site_col_name])]
dms2 <- remove.by.list(dm, samples_with_site_variable)



dms <- dms2
treatment <- dms$treatment
# get mean values of lat and long per species site 

# # Original Site Summary
unfiltered_site_summary <- dms2$meta$analyses %>% as.data.frame()%>%
  group_by(!!sym(species_col_name), !!sym(site_col_name)) %>%
  summarize(n_unfiltered = sum(n()),
            lat = mean(as.numeric(lat), na.rm=TRUE),
            long = mean(as.numeric(long),na.rm=TRUE),
            .groups = 'drop') %>%
  filter(n_unfiltered > 0) %>%
  as.data.frame()




### Identify and remove clones  ################################### 
# find and remove clones using SNPrelate kinship 
# then keep the ramet with the lowest missingnenss for each genet

#calculate kinship by population 
# VERY important that the population groups are true genetic groups and not conglomerates of multiple genetic groups
kin <- individual_kinship_by_pop(dms, RandRbase, species, dataset, dms$meta$analyses[,species_col_name], maf=0.1, mis=locus_miss, as_bigmat=TRUE)
kin[is.na(kin)] <- 0
# Finding the clones
kin3 <- ifelse(kin < clonal_threshold, 0, 1)

# cluster the clones 
network <- graph_from_adjacency_matrix(kin3, mode="undirected", diag=F,weighted=T) #makes the network based on k>0.4

ceb <- cluster_fast_greedy(network) # makes cluster groupings

# get clones 
clones <-as.data.frame(cbind(genet=ceb$membership, sample=ceb$names)) #get the clones from the network as a df
clones_out <- merge(clones, dms$meta$analyses[,c("sample","lat","long",site_col_name,species_col_name)], by="sample") #add some metadata
clones_out <- clones_out[order(as.numeric(clones_out$genet)),] #order the table by genet 
clones_out$genet <- as.numeric(clones_out$genet)

write.xlsx(clones_out, paste0(species,"/outputs_",site_col_name,"_",species_col_name,"/tables/PLINK_clones.xlsx"), asTable = FALSE, overwrite = TRUE)


# Now remove clones from the dataset for future analyses, keeping the sample which is the 
clonenum <- which(duplicated(clones_out$genet)==TRUE) 
clonenum2 <- unique(clones_out$genet[clonenum]) #identify which genets have multiple ramets


if (isFALSE(length(clonenum2)>0)){
  print("No genets with multiple ramets detected")
} else{
  keepsamps <- c()
  for (w in 1:length(clonenum2)){
    samps <- clones_out$sample[which(clones_out$genet==clonenum2[w])] #identify the NSW samples that are clones
    num_missing <- data.frame(matrix(ncol=2, nrow=length(samps)))
    colnames(num_missing) <- c("NSWNum", "sumNA")
    for (s in 1: length(samps)){
      num <- which(dms$sample_names==samps[s])
      num_missing$NSWNum[s] <- samps[s]
      num_missing$sumNA[s] <- sum(is.na(dms$gt[s,])==TRUE)
    }
    num_missing <- num_missing[order(num_missing$sumNA),]   #now keep the sample with the fewest NA's for dms$gt
    keepsamps[w] <- num_missing$NSWNum[1]
    
  }
  
  print(paste0("identified ", length(keepsamps), " genets with multiple ramets. Keeping the following ramets in dataset: ", list(keepsamps)))
  
  noclonegenets <- 1:max(clones_out$genet)
  noclonegenets <- which(!allgenets==clonenum2) #identify all genets that don't have multiple samples
  noclonesamps <- clones_out$sample[which(clones_out$genet %in% noclonegenets)] #these are NSW numbers with no clones identified. then add the clones we want to keep with these samples
  
  SampstoKeep <- c(keepsamps, noclonesamps) #this is our final list of samples which have removed clones and selected ramets of each genet with the lowest number of NA loci in dms$gt
  
  dms2 <- remove.by.list(dm, SampstoKeep)
  
  dms <- dms2
  treatment <- dms$treatment
  # get mean values of lat and long per species site 
  
  # # Original Site Summary
  unfiltered_site_summary <- dms2$meta$analyses %>% as.data.frame()%>%
    group_by(!!sym(species_col_name), !!sym(site_col_name)) %>%
    summarize(n_unfiltered = sum(n()),
              lat = mean(as.numeric(lat), na.rm=TRUE),
              long = mean(as.numeric(long),na.rm=TRUE),
              .groups = 'drop') %>%
    filter(n_unfiltered > 0) %>%
    as.data.frame()
  
}


}

#### Distance from PCA centroid ####

#run your standard scripts to filter and generate a dms prior to running below:

##### Run PCA #####
analysis <- "site"
analysis2<- "sp"

library(RRtools); library(adegenet); library(plyr); library(spatstat)


nd_gl <- dart2gl(dms, RandRbase, species, dataset) # converts the cleaned data to genlight format
nd_pca <- glPca(nd_gl, nf = 5, parallel = FALSE) #this might take a while!! # nf = number of principal components to be retained 

scatter(nd_pca) # a quick check of PCA


sample_PC1_PC2 <- cbind(dms$meta$sample_names,
                        dms$meta$analyses[, analysis],
                        dms$meta$analyses[, analysis2],
                        dms$meta$lat, dms$meta$long,
                        nd_pca$scores[, 1], # PC1
                        nd_pca$scores[, 2], # PC2
                        nd_pca$scores[, 3]) # PC3


colnames(sample_PC1_PC2) <- c("names", "site", "sp", "lat", "long", "PC1", "PC2", "PC3")

names <- rownames(sample_PC1_PC2)
rownames(sample_PC1_PC2) <- NULL
data <- cbind(names, sample_PC1_PC2)

df <- data.frame(data, stringsAsFactors = FALSE)
df$PC1 <- as.numeric(df$PC1)
df$PC2 <- as.numeric(df$PC2)
df$PC3 <- as.numeric(df$PC3)
df$lat <- as.numeric(df$lat)
df$long <- as.numeric(df$long)

df2new <- df[order(-df$lat),]
df2new$site <- factor(df2new$site, levels = unique(df2new$site), ordered = TRUE)

rep <- ddply(df2new,
             .(site),
             summarise,
             lat = mean(lat),
             long = mean(long),
             PC1 = mean(PC1),
             PC2 = mean(PC2),
             PC3 = mean(PC3))

rep2 <- rep[order(-rep$lat),]
x <- length(unique(df2new$site))
rep2$nums_n_site <- paste0(1:x, ": ", rep2$site)
rep2$num_site <- 1:x
rep222 <- rep2
rep222$nums_n_site <- factor(rep222$nums_n_site, levels = unique(rep222$nums_n_site))
df3 <- merge(df, rep222[, c(1, 7, 8)], by = "site")
#f <- nd_pca$eig[nd_pca$eig > sum(nd_pca$eig / length(nd_pca$eig))]
#e <- round(f * 100 / sum(nd_pca$eig), 1)
#percvar <- cbind(e[1], e[2], e[3])
#colnames(percvar) <- c("PC1", "PC2", "PC3")


#Quick plot to see
library(rgl)
plot3d(df3$PC1,df3$PC2, df3$PC3, centroidCoords,col=rainbow(length(unique(df3$site))))


#devtools::install_github("AckerDWM/gg3D")
library("gg3D")
ggplot(df3, aes(x=PC1,y=PC2, z=PC3))+
  geom_point(aes(x=centroidCoords[1],y=centroidCoords[2], z=centroidCoords[3]), color="red", size=5)+
  theme_void() +
  axes_3D() +
  stat_3D()
  



##### find the centroid by calcualting the mean of all points per dimension ####
centroidCoords <- c(mean(df3$PC1), mean(df3$PC2), mean(df3$PC3))

library(spatstat)
#create box limits for x, y, z
boundingbox <- box3(c(min(df3$PC1), max(df3$PC1)), 
                    c(min(df3$PC2), max(df3$PC2)),
                    c(min(df3$PC3), max(df3$PC3)))

#convert x, y, z cordinates into a pp3 object
X <- pp3(df3$PC1, df3$PC2, df3$PC3,
         boundingbox)

#now make a pp3 object for the median centroid to compare
threeDmedian <- pp3(centroidCoords[1], centroidCoords[2], centroidCoords[3],
                    boundingbox)

df3$DistPointPCA_1_2_3 <- crossdist.pp3(X, threeDmedian)



##### Now average the dist from PCA centroid for each site (can be changed to average per sp, region etc...)####

uqsites <- unique(dms$meta$analyses[, analysis])


avgdf3 <- c()
for (r in 1:length(uqsites)){
  nums <- which(df3$site==uqsites[r])
  fs <- nums[1]
  avgdf3$site[r] <- df3$site[fs]
  avgdf3$DistPointPC1_2_3[r] <- mean(df3$DistPointPCA_1_2_3[nums])
  
}


avgdf3 <- data.frame(avgdf3)


write.table(avgdf3, file= paste0("C:/Users/dimon/rrspecies/",species,"PCA123CentroidDist_.csv"), sep=",", row.names=F)


